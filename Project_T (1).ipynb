{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load data\n",
        "with open(\"train.wp_source\", \"r\", encoding=\"utf-8\") as f:\n",
        "    prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "with open(\"train.wp_target\", \"r\", encoding=\"utf-8\") as f:\n",
        "    full_stories = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "# Create a mapping for one-to-many relationship\n",
        "prompt_to_stories = defaultdict(list)\n",
        "\n",
        "# Align stories with prompts\n",
        "prompt_index = 0\n",
        "for story in full_stories:\n",
        "    prompt_to_stories[prompts[prompt_index]].append(story)\n",
        "    if prompt_index < len(prompts) - 1:\n",
        "        prompt_index += 1  # Move to the next prompt if applicable\n",
        "\n",
        "# Create DataFrame\n",
        "data = []\n",
        "for prompt, stories in prompt_to_stories.items():\n",
        "    for story in stories:\n",
        "        data.append({\n",
        "            \"story_id\": f\"{len(data):05d}\",\n",
        "            \"prompt\": prompt,\n",
        "            \"full_story\": story\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the merged dataset\n",
        "df.to_csv(\"merged_writing_prompts.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Dataset successfully merged with {len(df)} entries!\")\n"
      ],
      "metadata": {
        "id": "owcMxqLOb3lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Writing Prompt Dataset (Ensure CSV file has \"prompt\" and \"story\" columns)\n",
        "df = pd.read_csv(\"merged_writing_prompts.csv\")\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "hMBLQdaeb4ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJWRrCYcqf2_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load Writing Prompt Dataset (Ensure CSV file has \"prompt\" and \"story\" columns)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Project /merged_writing_prompts.csv\")\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "HHh4OfkvrHdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "v8Nm7f42rYN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Enter your Hugging Face token here\n",
        "hf_token = \"YOUR KEY\"  # Replace this with your actual token\n",
        "\n",
        "# Login using the token\n",
        "login(token=hf_token)\n",
        "\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "VMsHO5ZcrMmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade datasets"
      ],
      "metadata": {
        "id": "HmR8OFJKP7jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "f7Y_pPqPQD-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# ✅ Optimized Zero-Shot Classifier Pipeline\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device_map=\"auto\",\n",
        "    batch_size=256,\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        ")\n",
        "\n",
        "# Genre options\n",
        "genres = [\"Horror\", \"Sci-Fi\", \"Mystery\", \"Fantasy\", \"Romance\", \"Adventure\"]\n",
        "\n",
        "# ✅ Step 1: Classify genres in batches for better speed\n",
        "def classify_genre_batch(examples):\n",
        "    results = classifier(examples['prompt'], candidate_labels=genres, truncation=True)\n",
        "    return {\"genre\": [res['labels'][0] for res in results]}\n",
        "\n",
        "# ✅ Step 2: Convert to Dataset for batch processing\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# ✅ Step 3: Classify genres in batch\n",
        "dataset = dataset.map(classify_genre_batch, batched=True, batch_size=256)\n",
        "\n",
        "# ✅ Step 4: Reduce dataset size with balanced genre distribution\n",
        "def balance_dataset(dataset, genres, n_samples=10000):\n",
        "    balanced_data = []\n",
        "    samples_per_genre = n_samples // len(genres)\n",
        "    for genre in genres:\n",
        "        genre_data = dataset.filter(lambda example: example['genre'] == genre).shuffle(seed=42).select(range(samples_per_genre))\n",
        "        balanced_data.append(genre_data)\n",
        "    return Dataset.from_dict({key: sum((d[key] for d in balanced_data), []) for key in balanced_data[0].features})\n",
        "\n",
        "dataset = balance_dataset(dataset, genres)\n",
        "\n",
        "# ✅ Step 5: Batch classify genre scores\n",
        "def batch_classify(examples):\n",
        "    results = classifier(examples['prompt'], candidate_labels=genres, truncation=True)\n",
        "    genre_scores = [\n",
        "        ', '.join([f\"{label}: {score*100:.1f}%\" for label, score in zip(res['labels'], res['scores'])])\n",
        "        for res in results\n",
        "    ]\n",
        "    return {\"genre_mix\": genre_scores}\n",
        "\n",
        "dataset = dataset.map(batch_classify, batched=True, batch_size=256)\n",
        "\n",
        "# ✅ Step 6: Convert back to DataFrame and display results\n",
        "df1 = dataset.to_pandas()\n",
        "print(df1[[\"prompt\", \"genre_mix\"]].head())\n"
      ],
      "metadata": {
        "id": "6ee7qbKBQJH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv(\"/content/drive/MyDrive/Project /gen_dataset.csv\",index=False)"
      ],
      "metadata": {
        "id": "0IBe-7rhQLO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load Emotion Classifier with Longformer (handles longer sequences)\n",
        "emotion_classifier = pipeline(\"text-classification\", model=\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Function to handle long text sequences\n",
        "def get_emotion(story):\n",
        "    emotions = emotion_classifier(story, truncation=True, max_length=4096)\n",
        "    return emotions[0][\"label\"]\n",
        "\n",
        "# Estimate time to complete\n",
        "start_time = time.time()\n",
        "\n",
        "df1[\"emotion\"] = df1[\"full_story\"].apply(get_emotion)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Time taken to complete: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Display results\n",
        "print(df1[[\"full_story\", \"emotion\"]].head())"
      ],
      "metadata": {
        "id": "5kY903-tQRhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()\n",
        "df1.count()"
      ],
      "metadata": {
        "id": "VYLVKR6oQbR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd1\n",
        "\n",
        "# Load Writing Prompt Dataset (Ensure CSV file has \"prompt\" and \"story\" columns)\n",
        "df2 = pd1.read_csv(\"/content/drive/MyDrive/Project /gen_dataset.csv\")\n",
        "\n",
        "# Display the first few rows\n",
        "print(df2.head())"
      ],
      "metadata": {
        "id": "vgA4KTLsQYbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd1\n",
        "\n",
        "# Load Writing Prompt Dataset (Ensure CSV file has \"prompt\" and \"story\" columns)\n",
        "df2 = pd1.read_csv(\"/content/drive/MyDrive/Project /gen_dataset.csv\")\n",
        "\n",
        "# Display the first few rows\n",
        "print(df2.head())"
      ],
      "metadata": {
        "id": "H6Jv9w_FQfNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch.multiprocessing as mp\n",
        "from datasets import Dataset\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# ✅ Set multiprocessing start method for CUDA compatibility\n",
        "mp.set_start_method('spawn', force=True)\n",
        "\n",
        "# ✅ Optimized Zero-Shot Classifier Pipeline with Accelerate\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device_map=\"auto\",\n",
        "    batch_size=256,\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        ")\n",
        "\n",
        "# Genre options\n",
        "genres = [\"Horror\", \"Sci-Fi\", \"Mystery\", \"Fantasy\", \"Romance\", \"Adventure\"]\n",
        "\n",
        "# ✅ Step 1: Classify genres in batches for better speed\n",
        "def classify_genre_batch(examples):\n",
        "    results = classifier(list(examples['prompt']), candidate_labels=genres, truncation=True)\n",
        "    return {\"genre\": [res['labels'][0] for res in results]}\n",
        "\n",
        "# ✅ Step 2: Convert to Dataset for batch processing\n",
        "dataset = Dataset.from_pandas(df2)\n",
        "\n",
        "# ✅ Step 3: Classify genres in batch\n",
        "dataset = dataset.map(classify_genre_batch, batched=True, batch_size=256, num_proc=1)\n",
        "\n",
        "# ✅ Step 4: Reduce dataset size with balanced genre distribution\n",
        "def balance_dataset(dataset, genres, n_samples=10000):\n",
        "    balanced_data = []\n",
        "    samples_per_genre = n_samples // len(genres)\n",
        "    for genre in genres:\n",
        "        genre_data = dataset.filter(lambda example: example['genre'] == genre).shuffle(seed=42)\n",
        "        if len(genre_data) < samples_per_genre:\n",
        "            genre_data = genre_data.select(range(len(genre_data)))\n",
        "        else:\n",
        "            genre_data = genre_data.select(range(samples_per_genre))\n",
        "        balanced_data.append(genre_data)\n",
        "    return Dataset.from_dict({key: sum((d[key] for d in balanced_data), []) for key in balanced_data[0].features})\n",
        "\n",
        "dataset = balance_dataset(dataset, genres)\n",
        "\n",
        "# ✅ Step 5: Batch classify genre scores\n",
        "def batch_classify(examples):\n",
        "    results = classifier(list(examples['prompt']), candidate_labels=genres, truncation=True)\n",
        "    genre_scores = [\n",
        "        ', '.join([f\"{label}: {score*100:.1f}%\" for label, score in zip(res['labels'], res['scores'])])\n",
        "        for res in results\n",
        "    ]\n",
        "    return {\"genre_mix\": genre_scores}\n",
        "\n",
        "# ✅ Step 6: Final batch classification with optimized batch size\n",
        "dataset = dataset.map(batch_classify, batched=True, batch_size=256, num_proc=1)\n"
      ],
      "metadata": {
        "id": "WJx_1ufBQh6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = df2['genre'].value_counts()\n",
        "print(label_counts)"
      ],
      "metadata": {
        "id": "uHyK8r22Qm7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.to_csv(\"/content/drive/MyDrive/Project /gen_dataset_2.csv\",index=False)"
      ],
      "metadata": {
        "id": "QnG_2dVEQpoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd1\n",
        "\n",
        "# Load Writing Prompt Dataset (Ensure CSV file has \"prompt\" and \"story\" columns)\n",
        "df3 = pd1.read_csv(\"/content/drive/MyDrive/Project /gen_dataset_2.csv\")\n",
        "\n",
        "# Display the first few rows\n",
        "print(df3.head())"
      ],
      "metadata": {
        "id": "wSvpgfZ-Qs8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-xla -f https://storage.googleapis.com/libtpu-releases/index.html"
      ],
      "metadata": {
        "id": "Hd1Xl3EHQvKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load Emotion Classifier with Longformer (handles longer sequences)\n",
        "emotion_classifier = pipeline(\"text-classification\", model=\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Function to handle long text sequences\n",
        "def get_emotion(story):\n",
        "    emotions = emotion_classifier(story, truncation=True, max_length=4096)\n",
        "    return emotions[0][\"label\"]\n",
        "\n",
        "# Estimate time to complete\n",
        "start_time = time.time()\n",
        "\n",
        "df3[\"emotion\"] = df3[\"full_story\"].apply(get_emotion)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Time taken to complete: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Display results\n",
        "print(df3[[\"full_story\", \"emotion\"]].head())"
      ],
      "metadata": {
        "id": "tCceqjPkQx1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load Emotion Classifier optimized for A100 with FP16 and batch inference\n",
        "emotion_classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"bhadresh-savani/bert-base-uncased-emotion\",\n",
        "    device=0,  # Send to GPU\n",
        "    batch_size=256,  # Higher batch size for A100 efficiency\n",
        "    torch_dtype=\"auto\"  # Automatically uses float16 for A100\n",
        ")\n",
        "\n",
        "# Function to classify emotion\n",
        "def get_emotion(stories):\n",
        "    emotions = emotion_classifier(stories, truncation=True, max_length=512)\n",
        "    return [emotion['label'] for emotion in emotions]\n",
        "\n",
        "# Apply to DataFrame\n",
        "df3[\"emotion\"] = get_emotion(df3[\"full_story\"].tolist())\n",
        "\n",
        "print(df3[[\"full_story\", \"emotion\"]].head())\n"
      ],
      "metadata": {
        "id": "vtEbA0g_Q11u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = df3['emotion'].value_counts()\n",
        "print(label_counts)"
      ],
      "metadata": {
        "id": "XFD6dVxuQ9Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.to_csv(\"/content/drive/MyDrive/Project /emo_dataset1.csv\",index=False)"
      ],
      "metadata": {
        "id": "js_QnbjNRKn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd1\n",
        "\n",
        "# Load Writing Prompt Dataset (Ensure CSV file has \"prompt\" and \"story\" columns)\n",
        "df4 = pd1.read_csv(\"/content/drive/MyDrive/Project /emo_dataset1.csv\")\n",
        "\n",
        "# Display the first few rows\n",
        "print(df4.head())"
      ],
      "metadata": {
        "id": "mux_HWWzRDoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4[\"full_story\"] = df4[\"full_story\"].fillna(\"\").astype(str)"
      ],
      "metadata": {
        "id": "oeHFX7kYRL-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import unicodedata\n",
        "import time\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "DFz_0T-jROlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Writing Prompt Dataset (Ensure CSV file has \"prompt\" and \"story\" columns)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Project /emo_dataset1.csv\")\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "5DvDuwmhRVJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for empty or problematic entries\n",
        "print(df['full_story'].isna().sum())  # Count empty values\n",
        "print(df['full_story'].apply(len).describe())  # Check text length distribution"
      ],
      "metadata": {
        "id": "jS2kKycbRwoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=0, truncation=True)\n",
        "path_generator = pipeline(\"text-generation\", model=\"gpt2\", device=-1)  # Offloaded to CPU"
      ],
      "metadata": {
        "id": "LKd6aMLIR0fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_choices(story):\n",
        "    try:\n",
        "        # Generate the summary and format choices\n",
        "        summary = summarizer(story, max_length=200, min_length=100, do_sample=False)\n",
        "        summarized_text = summary[0]['summary_text']\n",
        "        return [\n",
        "            f\"Investigate: {summarized_text[:30]}...\",\n",
        "            f\"Ignore and move on: {summarized_text[30:]}...\"\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate_choices: {e}\")\n",
        "        return [\"Investigate: Error\", \"Ignore and move on: Error\"]"
      ],
      "metadata": {
        "id": "fYJSB4OKR_CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_story_path(story):\n",
        "    try:\n",
        "        # Truncate or preprocess story to avoid token limit issues\n",
        "        prompt = f\"Generate two creative paths for this scene based on the story: {story[:500]}\"\n",
        "\n",
        "        # Generate paths with specific configurations\n",
        "        response = path_generator(\n",
        "            prompt,\n",
        "            max_new_tokens=50,  # Limit the generated text length\n",
        "            num_return_sequences=2,  # Generate two separate outputs\n",
        "            pad_token_id=50256  # Ensure proper padding for GPT-2\n",
        "        )\n",
        "\n",
        "        # Extract and format the generated outputs\n",
        "        choice_1 = response[0]['generated_text']\n",
        "        choice_2 = response[1]['generated_text']\n",
        "        return {\"Choice 1\": choice_1.strip(), \"Choice 2\": choice_2.strip()}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate_next_story_path: {e}\")\n",
        "        return {\"Choice 1\": \"Path unavailable\", \"Choice 2\": \"Path unavailable\"}"
      ],
      "metadata": {
        "id": "OZcdWhx8SCcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_scene_description(story):\n",
        "    try:\n",
        "        summary = summarizer(story, max_length=60, min_length=20, truncation=True, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate_scene_description: {e}\")\n",
        "        return \"Scene description unavailable\""
      ],
      "metadata": {
        "id": "5XCwCeA1SEkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(story, max_token_limit=1024):\n",
        "    story = unicodedata.normalize(\"NFKD\", story.strip())\n",
        "    token_count = len(story.split())\n",
        "    if token_count > max_token_limit:\n",
        "        story = ' '.join(story.split()[:max_token_limit])\n",
        "    return story"
      ],
      "metadata": {
        "id": "EXIgFORfSHbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test generate_choices\n",
        "story = df.loc[4, \"full_story\"]\n",
        "print(\"Choices:\", generate_choices(story))\n",
        "\n",
        "# Test generate_next_story_path\n",
        "print(\"Next Story Path:\", generate_next_story_path(story))\n",
        "\n",
        "# Test generate_scene_description\n",
        "print(\"Scene Description:\", generate_scene_description(story))"
      ],
      "metadata": {
        "id": "fgMcqkFPSJpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "with tqdm(total=len(df), desc=\"Processing Stories\", unit=\"story\", dynamic_ncols=True) as pbar:\n",
        "    batch_size = 4  # Adjust batch size based on hardware\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        batch_df = df.iloc[i:i + batch_size].copy()  # Avoid SettingWithCopyWarning\n",
        "\n",
        "        for index, row in batch_df.iterrows():\n",
        "            story = row[\"full_story\"]\n",
        "\n",
        "            # Generate outputs and store them as strings\n",
        "            df.loc[index, \"choices\"] = str(generate_choices(story))\n",
        "            df.loc[index, \"next_story_path\"] = str(generate_next_story_path(story))\n",
        "            df.loc[index, \"scene_description\"] = str(generate_scene_description(story))\n",
        "\n",
        "        # Clear GPU cache after processing each batch\n",
        "        torch.cuda.empty_cache()\n",
        "        pbar.update(batch_size)"
      ],
      "metadata": {
        "id": "5jLa1i5HSMbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/Project /final.csv\",index=False)"
      ],
      "metadata": {
        "id": "MA8IjEGZSSzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Writing Prompt Dataset (Ensure CSV file has \"prompt\" and \"story\" columns)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Project /final.csv\")\n"
      ],
      "metadata": {
        "id": "Z6ijfV54SUZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "YESUTtUaUSjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "dCDRgGLpXuA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r storytelling_model.zip ./storytelling_model\n"
      ],
      "metadata": {
        "id": "VgRavzpTUzMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "QZ0-VgXGXxEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "# ✅ Load Mistral 7B model and tokenize\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ✅ Assign a padding token if missing\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ✅ Load model with correct device settings\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ✅ Load dataset (Replace with actual dataset path)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Project /final.csv\")\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(columns=[\"scene_description\"])\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# ✅ Split dataset into train & test\n",
        "split_dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "\n",
        "# ✅ Preprocessing function (Ensures fixed 1024-token sequences)\n",
        "def preprocess_function(examples):\n",
        "    inputs = (\n",
        "        f\"Prompt: {examples['prompt']}\\n\"\n",
        "        f\"Genre Mix: {examples['genre_mix']}\\n\"\n",
        "        f\"Emotion: {examples['emotion']}\\n\"\n",
        "        f\"Choices: {examples['choices']}\\n\"\n",
        "        f\"Story:\"\n",
        "    )\n",
        "    targets = f\"{examples['full_story']}\\nNext Story Path: {examples['next_story_path']}\"\n",
        "\n",
        "    # Tokenize with fixed length 1024 to avoid ArrowInvalid errors\n",
        "    tokenized_inputs = tokenizer(\n",
        "        inputs, truncation=True, padding=\"max_length\", max_length=1024, return_tensors=\"np\"\n",
        "    )\n",
        "    tokenized_targets = tokenizer(\n",
        "        targets, truncation=True, padding=\"max_length\", max_length=1024, return_tensors=\"np\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": tokenized_inputs[\"input_ids\"][0].tolist(),\n",
        "        \"labels\": tokenized_targets[\"input_ids\"][0].tolist()\n",
        "    }\n",
        "\n",
        "# ✅ Use `set_transform()` to apply preprocessing dynamically\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=False)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=False)\n",
        "\n",
        "# ✅ Convert dataset format to tensors for PyTorch compatibility\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
        "\n",
        "# ✅ LoRA Configuration (Efficient fine-tuning)\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention layers\n",
        "    lora_dropout=0.05\n",
        ")\n",
        "\n",
        "# ✅ Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ✅ Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral_story_gen\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"adamw_torch\",\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=3,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,  # Mixed precision for A100\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# ✅ Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# ✅ Train the model\n",
        "trainer.train()\n",
        "\n",
        "# ✅ Save fine-tuned model\n",
        "model.save_pretrained(\"./mistral_finetuned\")\n",
        "tokenizer.save_pretrained(\"./mistral_finetuned\")\n"
      ],
      "metadata": {
        "id": "NXejMpn9U1Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/drive/MyDrive/mistral_finetuned.zip\")"
      ],
      "metadata": {
        "id": "o0Qw12k7VHn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# ✅ Step 1: Load Tokenizer\n",
        "model_name = \"google/gemma-2b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ✅ Step 2: Load Model with Auto Device Mapping (Fixes Meta Device Issue)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Use FP16 for better efficiency\n",
        "    device_map=\"auto\"  # Automatically distributes model across GPU/CPU\n",
        ")\n",
        "\n",
        "# ✅ Step 3: Apply LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # LoRA for causal language modeling\n",
        "    r=16,  # Rank\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    lora_dropout=0.1,  # Dropout rate\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# ✅ Step 4: Ensure Model is in Training Mode\n",
        "model.train()\n",
        "\n",
        "# ✅ Step 5: Verify Device Allocation\n",
        "print(\"Model device allocation:\")\n",
        "print(model.hf_device_map)\n",
        "\n",
        "print(\"✅ Model is successfully loaded and ready for fine-tuning!\")\n"
      ],
      "metadata": {
        "id": "OEXxYRWRVLf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ✅ Ensure correct model path\n",
        "model_path = os.path.join(\"/content/drive/MyDrive/Project\", \"mistral_finetuned\")\n",
        "\n",
        "# ✅ Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# ✅ Load model efficiently\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True  # Reduces VRAM usage\n",
        ")\n",
        "\n",
        "# ✅ Ensure padding token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ✅ Define test input\n",
        "test_input = \"A mysterious figure enters the ancient ruins, carrying an old map. What happens next?\"\n",
        "\n",
        "# ✅ Tokenize input\n",
        "inputs = tokenizer(test_input, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "input_ids = inputs.input_ids.to(model.device)\n",
        "attention_mask = inputs.attention_mask.to(model.device)\n",
        "\n",
        "# ✅ Generate output with safe settings\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id  # Avoids potential warnings\n",
        "    )\n",
        "\n",
        "# ✅ Decode and print result\n",
        "generated_story = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated Story:\\n\", generated_story)\n"
      ],
      "metadata": {
        "id": "QTRXcpSqVvoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ✅ Define the correct local path for the fine-tuned model\n",
        "model_path = \"/content/drive/MyDrive/Project /mistral_finetuned\"  # Ensure this is the correct path\n",
        "\n",
        "# ✅ Load the fine-tuned model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# ✅ Ensure the pad token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ✅ Define test input\n",
        "test_input = \"A mysterious figure enters the ancient ruins, carrying an old map. What happens next?\"\n",
        "\n",
        "# ✅ Tokenize the input\n",
        "input_ids = tokenizer(test_input, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "# ✅ Generate output with max_new_tokens (instead of max_length)\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(input_ids, max_new_tokens=200, temperature=0.8, top_p=0.9, do_sample=True)\n",
        "\n",
        "# ✅ Decode and print generated story\n",
        "generated_story = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated Story:\\n\", generated_story)\n"
      ],
      "metadata": {
        "id": "VbGbHMQEVfhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# ✅ Define the correct local path for the fine-tuned model\n",
        "base_model_path = \"mistralai/Mistral-7B-v0.1\"  # Base model\n",
        "lora_model_path = \"/content/drive/MyDrive/Project /mistral_finetuned\"  # Fine-tuned LoRA model\n",
        "\n",
        "# ✅ Load the base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ✅ Load the fine-tuned LoRA model and merge\n",
        "model = PeftModel.from_pretrained(model, lora_model_path)\n",
        "model = model.merge_and_unload()  # Merges LoRA weights for inference\n",
        "\n",
        "# ✅ Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# ✅ Ensure the pad token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ✅ Define test input\n",
        "test_input = \"A mysterious figure enters the ancient ruins, carrying an old map. What happens next?\"\n",
        "\n",
        "# ✅ Tokenize the input\n",
        "input_ids = tokenizer(test_input, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "# ✅ Generate output with max_new_tokens (instead of max_length)\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(input_ids, max_new_tokens=200, temperature=0.8, top_p=0.9, do_sample=True)\n",
        "\n",
        "# ✅ Decode and print generated story\n",
        "generated_story = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated Story:\\n\", generated_story)\n"
      ],
      "metadata": {
        "id": "lBR9s9YXVi14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}